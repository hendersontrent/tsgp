---
title: "Introduction to tsgp"
author: "Trent Henderson"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 4
vignette: >
  %\VignetteIndexEntry{Introduction to tsgp}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 7,
  fig.width = 7,
  warning = FALSE,
  fig.align = "center"
)
```

```{r setup, message = FALSE, warning = FALSE}
library(tsgp)
library(ggplot2)
library(TSA)
```

## Purpose

`tsgp` is a lightweight package for modelling univariate time-series data with Gaussian processes (GP). It is important to remember that using a GP for time series basically converts the problem from one of modelling a *generative process* (e.g., if we used an [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) model) to that of essentially a curve-fitting exercise. GPs are a [Bayesian](https://en.wikipedia.org/wiki/Bayesian_statistics) method, so if you are unfamiliar with Bayesian inference, please go check out some resources on that, such as [this great video](https://www.youtube.com/watch?v=guTdrfycW2Q&list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&index=4) by Richard McElreath. Essentially, the key bits to remember for this post is that in Bayesian inference, we are interested in the posterior distribution, which is proportional to the product of the prior (our beliefs about an event before seeing the data) and the likelihood (the probability of the data given model parameters). Throughout this post, we are going to build a basic GP from scratch to highlight the intuition and mechanics underlying the modelling process.

## Generating some data

We are going to simulate some time series data with temporal dynamics that are commonly encountered in applied settings. Specifically, we are going to generate a noisy sine wave (i.e., to emulate periodic or seasonal data) with a linearly increasing trend:

```{r, message = FALSE, warning = FALSE, fig.width = 7, fig.height = 4}
set.seed(123)
y <- 3 * sin(2 * seq(0, 4 * pi, length.out = 100)) + runif(100) * 2
trend <- 0.08 * seq(from = 1, to = 100, by = 1)
y <- y + trend
tmp <- data.frame(timepoint = 1:length(y), y = y)

# Draw plot

ggplot(data = tmp) +
  geom_point(aes(x = timepoint, y = y), colour = "black") +
  geom_line(aes(x = timepoint, y = y), colour = "black") +
  labs(x = "Timepoint",
       y = "Values") +
  theme_bw()
```

## Decomposing a time series for modelling

There are a great many ways to do time-series analysis. A lot of it hinges on the expected usage of the model, such as forecasting or classification. In our case, we are interested in constructing a model that can accurately capture temporal properties and generate sensible predictions on which to do inference on. One approach for doing this is known as [structural time series](https://www.sciencedirect.com/science/article/abs/pii/S0169716105800458). Basically, this involves decomposing a time series into the sum of simpler parts. Such a model might take the following form:

$$
f(x) = f_{1}(x) + f_{2}(x) + \dots + f_{n}(x) + \epsilon
$$

where $f_{1}(x) \dots f_{n}(x)$ are the different *parts* of the time series we are explicitly modelling, and $\epsilon$ is the error term. Examples of these parts may include:

* Trend --- Is there an average increase/decrease over time?
* [Autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation) structure --- How correlated are points to those that came before it? At which lag is this relationship meaningful (e.g., one point behind? Five points behind?)?
* Seasonality --- Are there recurring periodicities in the data (e.g., cyclic patterns observed at a weekly and monthly level for retail sales)?
* Holidays --- Are there infrequent holiday periods which are important to capture? *We are going to ignore this here for simplicity*
* [Likelihood](https://en.wikipedia.org/wiki/Likelihood_function) --- What probability distribution best describes my data (e.g., for continuous data this might be a Gaussian, for count data it might be Poisson or Negative Binomial, etc.)?

Hopefully at this point the intention here is reasonably clear --- we can model time series in an extremely flexible and transparent manner through the explicit modelling of various statistical processes describe by the data. While a GP doesn't look like the above equation, it is important to keep this idea of components/parts in mind, as it will become quite useful very soon after we unpack the mechanics of GPs.

## Gaussian processes

The most general GP definition is that they are a *generalisation of the multivariate normal distribution to an infinite number of dimensions.* That is not super helpful... Let's unpack it a bit. 

Recall that a Gaussian distribution is parameterised by two scalar values: (i) mean $\mu$; and (ii) variance $\sigma^{2}$ (or the standard deviation which is the square root of the variance). We can write it as:

$$\mathcal{N}(\mu, \sigma^{2})$$

We can increase the dimensionality of this univariate case to describe *multivariate Gaussian distributions*, by increasing the dimensionality of the parameters. This results in our mean becoming a mean vector $\mu$ and our variance becoming a *covariance matrix* $\Sigma$ (as we now need to describe both the variance of each variable and the covariance between them), written as: 

$$\textbf{X} \sim \mathcal{N}(\mu, \Sigma)$$

A GP, by its broad definition, is just a generalisation of this idea to an infinite number of dimensions. This means our mean vector and covariance matrix become a *mean *function* $m(x)$ and covariance *function* $k(x, x')$ (also called a 'kernel'). See how we have abstracted from scalar values to functions without changing the broad underlying idea? Putting this all together, we have the general form of a GP:

$$
f(x) = \mathcal{GP}(m(x), k(x, x'))
$$

Now, it turns out that for most applications the mean function plays a relatively minor role in contributing to the outputs of a GP. In other words, almost all of the details are driven by the covariance function. Practically, this means that oftentimes $m(x)$ is just set to $0$. In contrast, by specifying different covariance functions, we can exercise substantial control over the resulting model fit and predictions. This is where domain and statistical expertise comes in.

There are a great number of covariance functions (noting that they **must** be [positive definite](https://mathworld.wolfram.com/PositiveDefiniteMatrix.html) to be valid) available to us, and deciding on which of them to use is a key aspect of the modelling process. In the GP regression context, we specify a *prior* over functions using the kernel. The process of Bayesian updating then occurs, resulting from the considerations of both our prior and the likelihood (the data). 

The following sections explore a few of the most common kernels you might encounter. To demonstrate them, we are just going to generate a bunch of data points in a constrained range (don't worry, we'll return to our original time series soon!):

```{r, message = FALSE, warning = FALSE}
x1 <- seq(from = -2, to = 2, length.out = 100)
```

### Exponentiated quadratic kernel

This is by far the most common kernel (also known as "radial basis function" or RBF or "squared exponential"), and even forms the basis for simpler machine learning methods, such as RBF [support vector machines](https://en.wikipedia.org/wiki/Support-vector_machine) (SVM). One of the reasons for this is it is quite similar to the formula for a univariate Gaussian distribution. We can write it mathematically as:

$$
k(x, x') = \text{exp} \left( -\frac{1}{2\sigma^{2}}||x - x'||^{2} \right)
$$

where $\sigma^{2}$ is the variance (average distance of a function away from its mean) and $\mathcal{l}$ is the lengthscale (the length of the "wiggles" in a function). You can refer to these as *hyperparameters* that we can set or learn to control the shape of the kernel.

In `tsgp`, this kernel is accessed via the function `cov_exp_quad` which returns an object of class `GPCov` (which is just a matrix but with a new inherited class that allows us to make use of R's generic functions). We can visualise the  matrix of this function on a subset of datapoints from our vector `x1`. Let's say the first 5 points for clarity:

```{r, message = FALSE, warning = FALSE}
mat_exp_quad <- cov_exp_quad(x1, x1, 1, 1)
mat_exp_quad[1:5, 1:5]
```   

We can see ones on the diagonal (variance with itself) and numbers everywhere else (covariance). Beautiful. We can also confirm that it is symmetrical:

```{r, message = FALSE, warning = FALSE}
isSymmetric(mat_exp_quad)
``` 

Since generics have been defined for classes constructed in `tsgp`, we can easily call `plot()` on our covariance matrix to generate some random samples from the kernel. This plot type is called a `"prior"` plot in `tsgp`. Let's try with 5 draws:

```{r, message = FALSE, warning = FALSE}
plot(mat_exp_quad, type = "prior", k = 5)
``` 

Notice how these curves could realistically be using to model a long-term effect (even non-linear ones), such as a trend? You might also notice that they are essentially shaped like [splines](https://en.wikipedia.org/wiki/Spline_(mathematics)) (similar to what is used in [generalised additive models](https://en.wikipedia.org/wiki/Generalized_additive_model)). 

As another example, assume that for our data we believed the pattern to be highly non-linear with substantial variation. Well, we can make the lengthscale ($\mathcal{l}$) shorter to increase the function "wiggliness" (yes, that's the technical term). Let's try $\mathcal{l} = 0.3$ instead of $\mathcal{l} = 1$ and see what happens:

```{r, message = FALSE, warning = FALSE}
mat_exp_quad2 <- cov_exp_quad(x1, x1, 1, 0.3)
plot(mat_exp_quad2, type = "prior", k = 5)
```

Our resulting functions are now far more wiggly. Notice how the statistical implementation follows directly from the conceptual understanding of the data? This is the idea at the heart of interpretable modelling.

Another way to visualise the covariance matrix is through a heatmap (`type = "matrix"` in `tsgp`). Here is the plot for all hyperparameters set to $1$:

```{r, message = FALSE, warning = FALSE}
plot(mat_exp_quad, type = "matrix")
```

And with the shorter lengthscale of $\mathcal{l} = 0.3$ for comparison:

```{r, message = FALSE, warning = FALSE}
plot(mat_exp_quad2, "matrix")
```

## Rational quadratic kernel

Another common kernel is the rational quadratic. You can think of this as being equivalent to adding together many squared exponential kernels with different lengthscales (credit to [this great resource](https://www.cs.toronto.edu/~duvenaud/cookbook/) for the brief description).

$$
k(x, x') = \sigma^{2} \left( 1 + \frac{||x - x'||^{2}}{2\alpha \mathcal{l}^{2}} \right)^{-\alpha}
$$

where where $\sigma^{2}$ is the variance, $\mathcal{l}$ is the lengthscale, and $\alpha$ is the mixing coefficient (where $\alpha > 0$).

In `tsgp`, this kernel is accessed via the function `cov_rat_quad` which again (and like all covariance functions in the package) returns an object of class `GPCov`.

Here are some random samples from the kernel:

```{r, message = FALSE, warning = FALSE}
mat_rad_quad <- cov_rat_quad(x1, x1, 1, 1, 1)
plot(mat_rad_quad, type = "prior", k = 5)
```

Note that again we can control wiggliness through lengthscale $\mathcal{l}$ but also now $\alpha$ here too. Here is a reduction in $\mathcal{l}$:

```{r, message = FALSE, warning = FALSE}
mat_rat_quad2 <- cov_rat_quad(x1, x1, 1, 1, 0.3)
plot(mat_rad_quad2, type = "prior", k = 5)
```

And here is a reduction in $\alpha$:

```{r, message = FALSE, warning = FALSE}
mat_rat_quad3 <- cov_rat_quad(x1, x1, 1, 0.1, 1)
plot(mat_rad_quad3, type = "prior", k = 5)
```

Finally, we can visualise the heatmap of the matrix with all hyperparameters set to $1$:

```{r, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 6}
plot(mat_rat_quad, "matrix")
```

And with $\alpha = 0.1$ as a comparison:

```{r, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 6}
plot(mat_rat_quad3, "matrix")
```

## Periodic kernel

Since we are interested in time series in this post, it would make sense to introduce some kernels which capture properties that are common in temporal data. One of these is seasonality (i.e., broad periodic patterns which repeat at regular intervals). We can capture this through a periodic kernel (also known as an 'exponentiated sine squared' kernel):

$$
k(x, x') = \sigma^{2} \text{exp} \left( -\frac{2}{\mathcal{l}^{2}}\text{sin}^{2} \left( \pi\frac{|x - x'|}{p} \right) \right)
$$

where $\sigma^{2}$ is the variance (or amplitude in this case), $\mathcal{l}$ is the lengthscale, and $p$ is the period (distance between repetitions). We can sometimes determine $p$ beforehand by either eyeballing the time-series plot, or numerically decomposing the data and determining periods using a [Fourier transform](https://en.wikipedia.org/wiki/Fourier_transform). We will just use $p = 1$ here for demonstration.

In `tsgp`, this kernel is accessed via the function `cov_periodic`.

And some random samples from the kernel:

```{r, message = FALSE, warning = FALSE}
mat_periodic <- cov_periodic(x1, x1, 1, 2, 1)
plot(mat_periodic, type = "prior", k = 5)
```

Notice the cyclic nature? This is what enables us to capture seasonality in time series. 

Here is the heatmap (how cool is it seeing periodicity in this format!):

```{r, message = FALSE, warning = FALSE}
plot(mat_periodic, "matrix")
```

## White noise kernel

The last common kernel we will introduce here is the white noise kernel. This does exactly as you might expect --- incorporates independent and identically-distributed (i.i.d.) noise into the GP. Mathematically, it is simple:

$$
k(x, x') = \sigma^{2}I_{n}
$$

where $\sigma^{2}$ is the variance of the noise and $I_{n}$ is the [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix) (a matrix of ones on the diagonal and zeros everywhere else --- kind of like the number one but for matrices). This kernel produces a matrix that has zeros everywhere except the diagonal, which now contains the variances.

In `tsgp`, this kernel is accessed via the function `cov_noise`.

And some random samples from the kernel:

```{r, message = FALSE, warning = FALSE}
mat_white_noise <- cov_noise(x = x1, sigma = 0.01)
plot(mat_white_noise, type = "prior", k = 5)
```

And the heatmap (with zero highlighting, as we would expect from white noise):

```{r, message = FALSE, warning = FALSE}
plot(mat_white_noise, "matrix")
```

*Side note: You may have picked up that every covariance function introduced here has the parameter * $\sigma^{2}$ *out front. This parameter acts as a scaling factor. By increasing its value, we can increase the contribution of a given covariance function to the model.*

# In the time series context

Now for the key piece of information: 

**GPs are not limited to a single kernel. We can combine kernels through multiplication or addition to generate a final kernel that captures all the dynamics as we need.** 

Here is where the structural time series decomposition idea comes in. Using GPs, we can apply the same logic: summing or multiplying different kernels that capture different statistical properties of our data can produce a model that appropriately understands the various dynamics. Summation of kernels can be thought of as an *OR* procedure, while multiplication can be thought of as an *AND* procedure. Looking at our original time-series data again, we have a linearly increasing trend, seasonality, and noise. If we can define a kernel that defines and combines these individual components, we *should* be able to construct a GP that can model the data with reasonable accuracy.

## Preparing the covariance function

We will start with the trend kernel. We know in our case that the trend itself linear, but for posterity (pun intended), we will use a squared exponential kernel anyway as it can model long-term smooth effects that may not necessarily be linear in other applications. Recall we coded this earlier as `cov_exp_quad`.

Next we will add a periodic kernel, as we know there is periodicity in our data. In our case, since we simulated the data, we know the period ($p$ in the kernel) is $2 \times 4 \times \pi \approx 25$, so we can be pretty confident that $p = 25$ is a good option. In practice, we would need to numerically determine the period (or maybe 2 or more if we have multiple seasonalities) using something like a Fourier transform. 

We are also going to add a white noise kernel as we know there is some error around the data (as we added noise to begin with through `runif`). We can now compute the three covariance matrices, and sum them for a final matrix which captures all the individual dynamics:

```{r, message = FALSE, warning = FALSE, eval = FALSE}
Sigma_exp_quad <- cov_exp_quad(y, y, 1, length(y))
Sigma_periodic <- cov_periodic(y, y, 1, 1, 25)
Sigma_white_noise <- cov_noise(sigma = 0.01, 1:nrow(Sigma_exp_quad))
Sigma_comb <- Sigma_exp_quad + Sigma_periodic + Sigma_white_noise
```

As a bonus, I mentioned earlier we can detect periods in data using a Fourier transform. Here is one example of how we can find the most important period in our data using a Fourier transform from the `TSA` package:

```{r, message = FALSE, warning = FALSE}
peri <- TSA::periodogram(y, plot = FALSE) # Do Fourier transform to decompose signal
dd <- data.frame(freq = peri$freq, spec = peri$spec) # Extract frequency and spectral value
order <- dd[order(-dd$spec),] # Rearrange to descending order
time <- 1 / order$f # Convert frequency to original scale (i.e., "timepoint" index)
time <- time[!time %in% length(y)] # Remove length of y from as it's not informative
time[1]
```

As we see, we get $p = 25$, the same period value we knew from the parameters of our simulated data. Hopefully this demonstrates to you how we can numerically test for periods on unknown data.

## Predicting from the posterior

To reiterate, we are trying to predict $\mathbf{y}_{2} = f(X_{2})$ points for $n_{2}$ new samples using our GP prior and $n_{1}$ datapoints ($(X_{1}, \mathbf{y}_{1})$):

$$
\left[\begin{array}{c} \mathbf{y}_{1} \\ \mathbf{y}_{2} \end{array}\right]
\sim
\mathcal{N} \left(
\left[\begin{array}{c} \mu_{1} \\ \mu_{2} \end{array}\right],
\left[ \begin{array}{cc}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{array} \right]
\right)
$$

We can summarise this using conditional probability for the posterior distribution: $p(\mathbf{y}_{2} | \; \mathbf{y}_{1}, X_{1}, X_{2})$.

Remember that $\mu_{1} = m(X_{1})$ (mean function of $n_{1} \times 1$ size) and $\Sigma_{11} = k(X_{1}, X_{1})$ (covariance function of size $n_{1} \times n_{1}$) specifies our GP. This can be broken down further to get the [conditional distributions](https://en.wikipedia.org/wiki/Conditional_probability), but I will save you the linear algebra (see [this post](https://peterroelants.github.io/posts/gaussian-process-tutorial/) for a nice overview). However, it is this linear algebra that makes GPs tricky to apply in practice, especially when our input data is very large. GPs rely on computing the [inverse](https://en.wikipedia.org/wiki/Invertible_matrix) of covariance matrices, pushing their computation time to $\mathcal{O}(N^{3})$ (compared to $\mathcal{O}(N)$ for standard Bayesian linear regression). Keep this in mind if you decide to use GPs in your own work.

Before we define the GP itself, for ease and clarity, we are going to define a simple function which automatically computes and sums the covariance functions we want. Note that the actual subsequent GP function actuall adds noise itself for computational stability reasons.

```{r, message = FALSE, warning = FALSE}
CovSum <- function(xa, xb, sigma_1 = 1, sigma_2 = 1, l_1 = 1, l_2 = 1, p = 1){
  Sigma_exp_quad <- cov_exp_quad(xa, xb, sigma_1, l_1)
  Sigma_periodic <- cov_periodic(xa, xb, sigma_2, l_2, p)
  Sigma <- Sigma_exp_quad + Sigma_periodic
  return(Sigma)
  }
```

Okay, we can now use the core `GP` function in `tsgp` to calculate the posterior mean vector and covariance matrix. We are going to predict values for a set of evenly-spaced time points across the $[1-100]$ domain. `GP` takes the following arguments:

* `x` --- xx
* `xprime` --- xx
* `y` --- xx
* `covfun` --- xx
* `sigma_noise` --- xx

```{r, message = FALSE, warning = FALSE}
mod <- GP(x1, 1:length(y), y, CovSum, 0.5)
```

x

```{r, message = FALSE, warning = FALSE}

```
